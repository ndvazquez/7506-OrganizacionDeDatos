{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import Levenshtein\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/no-third-party-conversions.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de términos de búsqueda\n",
    "\n",
    "En este notebook, vamos a analizar los términos de búsqueda ingresados por los usuarios del sitio de la empresa.\n",
    "\n",
    "Antes de realizar cualquier tipo de observación, será necesario limpiar los datos un poco, ya que los errores ortográficos son moneda corriente, y podrían llegar a distorsionar lo que haremos más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrección de errores comunes.\n",
    "\n",
    "Nos ocuparemos de los errores comunes utilizando la distancia de Levenshtein como métrica para determinar cuándo estamos ante un error común.\n",
    "\n",
    "La distancia de Levenshtein se entiende como el número mínimo de operaciones que se deben realizar sobre una cadena para transformarla en otra. La misma se presenta como un valor entre 0 y 1, por lo que consideraremos que aquellas palabras que presenten un valor mayor o igual a 0.7 serán tomadas como un error y las corregiremos.\n",
    "\n",
    "Por supuesto esto presenta algunos problemas, como en el caso de palabras como `celular` y `celulares`, ya que si bien son distintas, una de ellas será considerada errónea y por lo tanto se modificará, por lo cual las excluiremos de la lista de errores comunes y sólo nos quedaremos con los nombres de celulares/marcas.\n",
    "\n",
    "Además quitaremos caracteres especiales ya que también podrían ser parte de los errores presentes en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_comunes = ['samsung', 'galaxy', 'iphone', 'apple', 'sony', 'plus']\n",
    "\n",
    "def corregir_errores_comunes(frase):\n",
    "    palabras = frase.split(' ')\n",
    "    for palabra in palabras:\n",
    "        for clave in errores_comunes:\n",
    "            if Levenshtein.ratio(clave, palabra) >= 0.7:\n",
    "                palabras[palabras.index(palabra)] = clave\n",
    "                break\n",
    "    return ' '.join(palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminos_busqueda = df.loc[df['search_term'].notnull(), ['timestamp', 'person', 'event', 'search_term', 'skus']]\n",
    "terminos_busqueda['timestamp'] = pd.to_datetime(terminos_busqueda['timestamp'])\n",
    "terminos_busqueda['search_term'] = terminos_busqueda['search_term'].apply(lambda x: x.lower())\n",
    "terminos_busqueda['search_term'] = terminos_busqueda['search_term'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "terminos_busqueda['search_term'] = terminos_busqueda['search_term'].apply(lambda x: corregir_errores_comunes(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando la frecuencia de los términos de búsqueda.\n",
    "\n",
    "Ahora que terminamos con la breve limpieza de los términos de búsqueda, utilizaremos dos visualizaciones para ver las frecuencias de los mismos.\n",
    "\n",
    "En primer lugar, un Wordcloud para tener una idea rápida de cuáles son los términos más frecuentes.\n",
    "\n",
    "Luego utilizaremos un gráfico de barras para visualizar los diez términos más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color='white',\n",
    "                      max_words=200\n",
    "                     ).generate(str(terminos_busqueda['search_term']))\n",
    "plt.figure(figsize=[14,4])\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot_st = terminos_busqueda['search_term'].value_counts().head(10)\n",
    "plt.figure(figsize=[12,6])\n",
    "g = sns.barplot(x=barplot_st.index, y=barplot_st.values, palette='hls')\n",
    "g.set_title('Top 10 búsquedas de usuarios', fontsize=20)\n",
    "g.set_xlabel('Búsqueda', fontsize=18)\n",
    "g.set_ylabel('Frecuencia', fontsize=18)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevancia del término de búsqueda para los resultados devueltos por el buscador de la página.\n",
    "\n",
    "Al ver que contabamos con la información relacionada a las búsquedas realizadas por los usuarios, surgió la idea de intentar medir qué tan relevante son los resultados devueltos por el buscador, para los términos de búsqueda más usados por los usuarios del sitio.\n",
    "\n",
    "Por supuesto se puede asumir que el buscador del sitio funciona correctamente, pero de todas formas creemos que puede ser un análisis interesante.\n",
    "\n",
    "Para esto utilizaremos una métrica conocida como `term frequency` la cual recompensa o adjudica mayor relevancia a aquellos términos que aparecen con mayor frecuencia en un texto.\n",
    "\n",
    "A diferencia del análisis de palabras claves en textos dentro de un conjunto de textos, donde se utiliza la métrica `inverse document frequency` en conjunto a `term frequency` para premiar a aquellas palabras que además de ser frecuentes, también son _raras_, aquí sólo nos interesa qué tan frecuentemente aparecen estos términos en los resultados de las búsquedas.\n",
    "\n",
    "En nuestro caso utilizaremos los resultados de cada búsqueda, y haremos un promedio de la frecuencia con la que aparece el término utilizado para obtener dichos resultados, de esta forma asignaremos los puntajes a cada uno de los términos de búsqueda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero obtendremos las características asociadas a cada SKU, ya que el resultado de una búsqueda es un listado de SKUs.\n",
    "\n",
    "Esto lo pasaremos a un diccionario para luego transformar ese listado de SKUs en cadenas con las características del modelo asociado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skus = df.loc[~df['sku'].isnull(), ['sku', 'model', 'condition', 'storage', 'color']].drop_duplicates(subset='sku')\n",
    "skus['sku'] = skus['sku'].apply(lambda x: str(x[:-2]))\n",
    "skus.set_index('sku', inplace=True)\n",
    "skus = skus.apply(lambda x: x.str.lower(), axis=1)\n",
    "skus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caracteristicas_segun_sku = skus.T.to_dict(orient='list')\n",
    "\n",
    "def skus_a_caracteristicas(skus):\n",
    "    skus_separados = str(skus).split(',')\n",
    "    res = [' '.join(caracteristicas_segun_sku.get(x, 'Desconocido')) for x in skus_separados]\n",
    "    return ' '.join(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora filtraremos aquellos términos que no tengan un listado de SKUs asociado y agregaremos una columna con las características de cada SKU devuelto por el buscador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminos_busqueda_sin_nulls = terminos_busqueda.dropna(subset=['skus'])\n",
    "terminos_busqueda_sin_nulls['resultados_busqueda'] = terminos_busqueda_sin_nulls['skus'].apply(skus_a_caracteristicas)\n",
    "terminos_busqueda_sin_nulls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente preparemos la columna `resultados_busqueda` para ser procesada en el cálculo de `td-idf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminos_busqueda_sin_nulls = terminos_busqueda_sin_nulls.groupby('search_term')\\\n",
    "                            .agg({'resultados_busqueda':'-'.join, 'event':'count'})\\\n",
    "                            .reset_index()\\\n",
    "                            .rename(columns={'event':'frecuencia'})\n",
    "terminos_busqueda_sin_nulls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto creo que vuela porque no tiene mucho sentido usarlo, por ahora lo dejo por las dudas.\n",
    "\n",
    "def calcular_tfidf(fila):\n",
    "    # Sumamos uno al denominador en las divisiones por las dudas.\n",
    "    res = 0\n",
    "    keyword_freq = 0\n",
    "    keyword = fila[0]\n",
    "    search_results = fila[1].split('-')\n",
    "    for elem in search_results:\n",
    "        if (keyword in elem):\n",
    "            keyword_freq += 1\n",
    "    idf = np.log((len(search_results) * 1.0) / (keyword_freq + 1))\n",
    "    for elem in search_results:\n",
    "        tf = (elem.count(keyword) * 1.0 ) / len(elem.split(' '))\n",
    "        res += tf*idf\n",
    "    return res / (len(search_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_tf(fila):\n",
    "    res = 0\n",
    "    keyword = fila[0]\n",
    "    search_results = fila[1].split('-')\n",
    "    for elem in search_results:\n",
    "        tf = (elem.count(keyword) * 1.0 ) / len(elem.split(' '))\n",
    "        res += tf\n",
    "    return res / (len(search_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_terminos = terminos_busqueda_sin_nulls.sort_values(by='frecuencia', ascending=False).head(100)\n",
    "top_100_terminos['tf_score'] = top_100_terminos.apply(lambda x: calcular_tf(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_terminos.sort_values(by='tf_score', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_terminos = top_100_terminos.head(10)\n",
    "plt.figure(figsize=[12,6])\n",
    "g = sns.barplot(data=bp_terminos, x='search_term', y='tf_score', palette='hls')\n",
    "g.set_title('Puntaje según relevancia del search term para con sus resultados', fontsize=18)\n",
    "g.set_xlabel('Término de búsqueda', fontsize=16)\n",
    "g.set_ylabel('Puntaje', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otro enfoque para analizar los términos de búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien los términos de búsqueda son un punto de interés para nuestro análisis, creemos que se puede analizar de otra forma. En particular, podríamos analizar la frecuencia de n-gramas en estas búsquedas, para agrupar mejor las mismas.\n",
    "\n",
    "### ¿Cuál es la motivación de este enfoque? \n",
    "\n",
    "Puesto que estamos analizando el input consumido por el search engine del sitio de la empresa, tal vez podamos encontrar algo interesante, como nuevas keywords para los productos ofrecidos en el sitio.\n",
    "\n",
    "### ¿Cómo vamos a realizar este análisis?\n",
    "\n",
    "En primer lugar, debemos filtrar las _stopwords_ del lenguaje que presenta mayoría (en este caso, portugués). El fin de esto es prevenir que aparezcan en nuestros reportes palabras de uso común pero que carecen de contenido, como por ejemplo `por`, `de`, `mais`.\n",
    "\n",
    "Luego utilizaremos la biblioteca `nltk` para poder procesar cada término de búsqueda y extraer los n-gramas que nos interesen.\n",
    "\n",
    "Finalmente presentaremos la información por medio de gráficos de barra, ya que nos interesa ver los n-gramas que presenten mayor frecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminos_sin_filtrar = terminos_busqueda['search_term'].tolist()\n",
    "\n",
    "errores_comunes = ['samsung', 'galaxy', 'iphone', 'sony', 'celular', 'plus', 'barato', 'usado']\n",
    "## Vamos a cachear las stopwords ya que queremos evitar un cuello de botella al cargarlo constantemente.\n",
    "cached_stopwords = stopwords.words('portuguese')\n",
    "\n",
    "terminos_filtrados = []\n",
    "\n",
    "for frase in terminos_sin_filtrar:\n",
    "    terminos_filtrados.append(' '.join([palabra for palabra in frase.split(' ') if palabra not in cached_stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_ngramas(frases, n):\n",
    "    data = []\n",
    "    for frase in frases:\n",
    "        tokens = [token for token in frase.split(\" \") if token != \"\"]\n",
    "        ngramas = list(ngrams(tokens, n))\n",
    "        for _ in ngramas:\n",
    "            data.append(_)\n",
    "    res = [(lambda x: ' '.join(x))(x) for x in data]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = obtener_ngramas(terminos_filtrados, 1)\n",
    "serie = pd.Series(x for x in unigrams).value_counts().head(10)\n",
    "plt.figure(figsize=[12,8])\n",
    "g = sns.barplot(x=serie.index, y=serie.values, palette='hls')\n",
    "g.set_title('Frecuencia de unigramas en términos de búsqueda', fontsize=18)\n",
    "g.set_xlabel('Unigramas', fontsize=16)\n",
    "g.set_ylabel('Frecuencia', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = obtener_ngramas(terminos_filtrados, 2)\n",
    "serie = pd.Series(x for x in bigrams).value_counts().head(10)\n",
    "plt.figure(figsize=[12,8])\n",
    "g = sns.barplot(x=serie.index, y=serie.values, palette='hls')\n",
    "g.set_title('Frecuencia de 2-gramas en términos de búsqueda', fontsize=18)\n",
    "g.set_xlabel('2-gramas', fontsize=16)\n",
    "g.set_ylabel('Frecuencia', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = obtener_ngramas(terminos_filtrados, 3)\n",
    "serie = pd.Series(x for x in trigrams).value_counts().head(10)\n",
    "plt.figure(figsize=[12,8])\n",
    "g = sns.barplot(x=serie.index, y=serie.values, palette='hls')\n",
    "g.set_title('Frecuencia de 3-gramas en términos de búsqueda', fontsize=18)\n",
    "g.set_xlabel('3-gramas', fontsize=16)\n",
    "g.set_ylabel('Frecuencia', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puntaje de cada n-grama\n",
    "\n",
    "Si bien sabemos que contamos con un subset del dataset completo, y no sabemos si el mismo es representativo de su _progenitor_, intentaremos puntuar a cada n-grama según las conversiones que se realizaron a partir de su aparición en un término de búsqueda.\n",
    "\n",
    "Además tampoco es posible realizar una reconstrucción certera del _funnel_ por el cual el usuario llega a realizar una conversión, sólo se puede aproximar. \n",
    "\n",
    "Analizaremos bigramas y trigramas en particular.\n",
    "\n",
    "TODO: Profundizar un poco más esta explicación ya que no está ni mencionado cómo se van a puntuar los n-gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_freq_conversiones(ngram):\n",
    "    \"\"\"\n",
    "    ### TODO: Falta filtrar por un delta de tiempo prudente.\n",
    "\n",
    "    usuarios_conversiones = set(df.loc[(df['event'] == 'conversion') & (df['person'].isin(usuarios)), 'person'])\n",
    "\n",
    "    presentan_conversiones = df.loc[((df['event'] == 'conversion') & (df['person'].isin(usuarios))) |\\\n",
    "                                   ((df['event'] == 'searched products') & (df['search_term'].str.contains(bigram)) &\\\n",
    "                                   (df['person'].isin(usuarios_conversiones)))\n",
    "                                   , ['timestamp', 'event', 'search_term', 'person', 'model', 'sku', 'skus']]\n",
    "    \"\"\"\n",
    "\n",
    "    busquedas_filtradas = df.loc[(df['event'] == 'searched products') & (df['search_term'].str.contains(ngram)), 'person']\n",
    "\n",
    "    usuarios = set(busquedas_filtradas)\n",
    "\n",
    "    res = df.loc[((df['event'] == 'conversion') & (df['person'].isin(usuarios))), 'model']\n",
    "    # Descartamos aquellos valores que estén muy lejos del ngram, según distancia de Levenshtein.\n",
    "    return res.apply(lambda x: x if Levenshtein.ratio(x.lower(), ngram) > 0.5 else None).dropna().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puntaje para cada  bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversiones_bigramas = pd.Series(x for x in bigrams).value_counts().to_frame().reset_index()\n",
    "conversiones_bigramas.rename(columns={'index':'bigrama', 0:'freq'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si bien el grado de dispersión es altísimo, tomaremos los 100 primeros elementos para asignar puntajes.\n",
    "conversiones_bigramas['conversiones'] = conversiones_bigramas['bigrama'].head(100)\\\n",
    ".apply(lambda x: obtener_freq_conversiones(x))\n",
    "conversiones_bigramas['puntaje'] = conversiones_bigramas['conversiones'] / conversiones_bigramas['freq'] * 100\n",
    "conversiones_bigramas.sort_values(by='puntaje', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversiones_bigramas.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puntaje para cada trigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversiones_trigramas = pd.Series(x for x in trigrams).value_counts().to_frame().reset_index()\n",
    "conversiones_trigramas.rename(columns={'index':'trigrama', 0:'freq'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá nos pasa lo mismo con el grado de dispersión, así que calcularemos puntajes por diversión.\n",
    "conversiones_trigramas['conversiones'] = conversiones_trigramas['trigrama'].head(100)\\\n",
    ".apply(lambda x: obtener_freq_conversiones(x))\n",
    "conversiones_trigramas['puntaje'] = conversiones_trigramas['conversiones'] / conversiones_trigramas['freq'] * 100\n",
    "conversiones_trigramas.sort_values(by='puntaje', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversiones_trigramas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
